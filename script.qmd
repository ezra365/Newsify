---
title: "Newsify"
format: 
    pdf: 
        toc: false
        toc-depth: 2
        number-sections: true
        code-block-border-left: true
        colorlinks: true
author: "Ezra Sharpe"
execute:
    engine: python
---

### Part 1 - Setup 

```{python}

import os
import json
from enum import Enum
import requests
from bs4 import BeautifulSoup

# Load API keys from a JSON file
def load_api_keys(filename):
    with open(filename, 'r') as file:
        keys = json.load(file)
    return keys

# Get the API key
api_keys = load_api_keys('key.json')
openai_api_key = api_keys.get('OPENAI_API_KEY')


def keys_available(api_key):
    # Checking it loaded succesfully
    return type(api_key)

# The API key should be a string, let's check without printing key
print(f"The API key type is: {keys_available(openai_api_key)}")

```

### Part 2 - Webscrape Articles

```{python}

import requests
import time as t
from bs4 import BeautifulSoup
from enum import Enum
from datetime import datetime, timedelta, date
from selenium import webdriver

# Change the URL above to any of TechCrunch's categories which suit your needs!
url = "https://techcrunch.com/category/security/" 

driver = webdriver.Chrome()
driver.get(url)

html = driver.page_source

driver.quit()

def __str__(self):
      return self.value

def get_text(detail_url):
    info = BeautifulSoup(requests.get(detail_url).text, 'lxml')
    article_content = info.select_one('div.article-content')
    if article_content:
        return '\n'.join([p.text.strip() for p in article_content.findChildren('p', recursive=False)])
    else:
        return None  # Or any appropriate value or indication that no content was found


def scrape(date = date.today()):
  
  soup = BeautifulSoup(html, "lxml")

  articles = soup.find_all('article', class_=["post-block", "post-block--image", "post-block--unread"])

  print('number of articles so far:', len(articles))


  print('#### input ###')
  print('date: ', date)
  print('#### === ### \n')
  data = []
  for article in articles[:10]: # limit to 10 articles for now, but feel free to change based on your needs
    t.sleep(2)
    item = {
        'title': article.h2.text.strip(),
        'url': article.a['href'],
        'text': get_text("https://techcrunch.com" + article.a['href'])
    }
    data.append(item)

  return data
    
date = date.today() # we want the news from today's date
data = scrape(date)
print('total articles:', len(data))

```

### Part 3 - Generate Summaries

```{python}

from IPython.display import Markdown
from langchain_openai import ChatOpenAI
from langchain.schema import (
    HumanMessage
)

def get_template(title, text):

  # prepare template for prompt and experiment with prompt engineering
  template = """You are an advanced ai assistant that summarizes online articles about developments in technology.

  Here's the article you need to summarize:
  ==========================
  Title: {article_title}

  Text: {article_text}
  ==========================

  Write a summary of the previous article in 200 words or less, with a particular focus on giving key details, main points, and examples directly from the article.
  """

  prompt = template.format(article_title=title, article_text=text)

  messages = [HumanMessage(content=prompt)]
  return messages

def get_summary(messages, openai_api_key):
    # instantiate model with API key
    chat = ChatOpenAI(api_key=openai_api_key, temperature=0)

    # generate summary
    summary = chat(messages)
    return summary.content


def get_output(article):
  title = article['title']
  url = article['url']
  text = article['text']

  messages = get_template(title, text)
  summary = get_summary(messages, openai_api_key)
  output = f"**{title}** \n\n {summary} [View Full]({url}) \n\n"
  return output

def summarize_articles(data):
  markdown_list = []
  for article in data:
    output = get_output(article)
    markdown_list.append(output)

  markdown_string = ''.join(markdown_list)
  return markdown_string

summaries = summarize_articles(data)
Markdown(summaries)

```

### Part 4 - give me feedback for iterative improvement!

```{python}

import requests

def submit_feedback(url, summary, rating, comment=''):
    feedback_data = {
        "url": url,
        "summary": summary,
        "rating": rating,
        "comment": comment
    }
    response = requests.post("https://newsify-dc426740efa8.herokuapp.com/submit_feedback", json=feedback_data)
    return response.json()

# Example usage
url = "https://example.com/article"
summary = "This is a summary of the article."
rating = "Excellent"
comment = "I found the summary very informative and well-written."
response = submit_feedback(url, summary, rating, comment)
print(response)

# heroku pg:psql - command to open postgresql interface in terminal 
```

open terminal
run python app.py in your terminal. You will see a server is running - keep it running
open script.qmd 
run the last chunk of code above. You don't need to run anything else 
Once youve succesfully run final chunk, you can end the terminal server (ctrl+c in terminal)

Done - thanks.